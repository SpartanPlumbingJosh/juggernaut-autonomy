# JUGGERNAUT AUTONOMOUS BUILDER AUDIT (2026-02-01)

## Scope
This audit documents what the system **actually does in production** vs what is claimed in docs/prompts, with an emphasis on the requirements to build an autonomous code-generating worker (“Autonomous Builder”).

Repos assessed:
- `juggernaut-autonomy` (engine + MCP + automation)
- `spartan-hq` (Neural Chat UI + proxy API; execution lives in engine)

Data sources:
- Production task-type distribution (provided by Josh)
- Code scan of engine, handlers, Brain tool loop, MCP server, verification, PR tracking/monitoring

---

## Production Reality (Smoking Gun)
Production tasks show strong success for analytics/research and weak success for code/automation:

- `analysis`: 302 total / 291 completed (96%)
- `evaluation`: 258 / 258 (100%)
- `scan`: 71 / 71 (100%)
- `research`: 23 / 20 (87%)

Code/automation task types below this line fail disproportionately:

- `code`: 20 / 6 (30%)
- `code_fix`: 13 / 0 (0%)
- `code_change`: 9 / 6 (67%)
- `automation`: 4 / 0 (0%)
- `code_implementation`: 1 / 0 (0%)

Interpretation:
- The system is strong at SQL-backed analytics and integration-backed research.
- The “autonomous builder” path is present in code, but not consistently reachable and completion semantics are leaky.

---

## Actual State (with file/function citations)

### 1) Modular handlers: strong for analysis/research/scan; weak for build artifacts
Handler registry:
- `juggernaut-autonomy/core/handlers/__init__.py` maps task types to handler classes.

Real handler implementations:
- `analysis` → `core/handlers/analysis_handler.py` (SQL-driven analytics)
- `database` → `core/handlers/database_handler.py` (read-only SQL; write requests require approval)
- `scan` → `core/handlers/scan_handler.py` (DB scanning; mixed real + placeholder scan types)
- `research` → `core/handlers/research_handler.py` (Perplexity + Puppeteer integrations; graceful degradation)
- `test` → `core/handlers/test_handler.py` (SQL-based verification suite)

Non-artifact handler path:
- `development`, `workflow`, `design`, etc. → `core/handlers/ai_handler.py` via registry
  - The `AIHandler` produces structured JSON responses but does not write files / open PRs.

### 2) A real code executor exists and creates PRs (and works)
Engine routing:
- `juggernaut-autonomy/main.py` `execute_task(...)`
  - Normalizes `code_fix`, `code_change`, `code_implementation` → `task_type="code"`
  - `task_type == "code"` runs the code executor path if `CODE_TASK_AVAILABLE`

Code executor:
- `juggernaut-autonomy/src/task_executor_code.py`
  - `execute_code_task(...)` → `CodeTaskExecutor.execute(...)`
  - Generates code via `src/code_generator.py` (OpenRouter)
  - Commits files + opens PRs via `src/github_automation.py` (GitHub REST API)

Production examples (provided):
- `cbba081f` (failed): executor succeeded, PR created (#257), then task failed when PR was closed without merge.
- `713eea1e` (completed): executor created PR (#242), `merged: false`, but task still marked `completed`.

### 3) PR lifecycle monitoring exists and is strict (merged-only completion)
PR monitor job:
- `juggernaut-autonomy/src/scheduled/pr_monitor.py` `monitor_pending_prs(...)`
  - Selects tasks in `status='awaiting_pr_merge'` with `pr_tracking.current_state NOT IN ('merged','closed')`
  - If GitHub reports merged → sets task `status='completed'` and writes `completion_evidence` type `pr_merged`
  - If PR closed → sets task `status='failed'` with `error_message='PR closed without merge'`

PR tracker:
- `juggernaut-autonomy/core/pr_tracker.py` (`PRTracker`)
  - Tracks PRs in `pr_tracking`
  - Reads state via GitHub API

### 4) Evidence theater leak: PR creation is treated as “valid evidence”
Completion verifier:
- `juggernaut-autonomy/core/verification.py` `CompletionVerifier.verify_evidence(...)`

Behavior:
- If evidence contains a PR URL/number:
  - If GitHub confirms merged → returns `(True, 'pr_merged')`
  - Else (not merged OR cannot confirm) → returns `(True, 'pr_created')`

Implication:
- A task can be marked `completed` with evidence `pr_created` (PR opened but not merged), depending on the status-update path.

Status update & gating:
- `juggernaut-autonomy/main.py` `update_task_status(...)`
  - Builds `completion_evidence` from `result_data` and PR URLs
  - Uses `CompletionVerifier` to decide if evidence is valid

Result:
- There are multiple completion pathways.
  - Strict: `awaiting_pr_merge` + `pr_monitor` completes only when merged.
  - Leaky: direct status updates can set `completed` with `pr_created`.

### 5) Neural Chat tool loop exists, but its tool schema blocks code-writing
Brain loop:
- `juggernaut-autonomy/core/brain.py` (`BrainService`)
  - Tool-calling loop executes tools via MCP (`_execute_tool(...)`)

MCP server tool surface:
- `juggernaut-autonomy/mcp/server.py`
  - Includes `github_put_file`, `github_merge_pr`, Railway deploy mutations, etc.

But Brain tool exposure is restricted:
- `juggernaut-autonomy/core/mcp_tool_schemas.py` defines the tools the LLM can call.
  - This schema does not expose all write-capable tools.

Consequence:
- Neural Chat can appear to have “full repo management” but cannot always take the write actions needed to create artifacts.

---

## Critical Gaps for Autonomous Builder

### Gap A: Tool schema mismatch (capability exists but is not callable)
- MCP includes GitHub write primitives, but Brain’s tool schema omits key write paths.
- Fixing this is “wiring”, not rebuilding.

### Gap B: Two parallel stacks (MCP tool loop vs direct code executor)
- MCP stack: Brain → MCP tools
- Code stack: `main.py` → `src/task_executor_code.py` → GitHub/OpenRouter

Because they are not unified:
- Neural Chat cannot reliably trigger the best code pipeline.
- Capability claims drift from reality.

### Gap C: Completion semantics (PR created ≠ task completed)
- The verifier treats `pr_created` as “valid evidence”, enabling `completed` without merge.
- This corrupts success metrics and produces “done” tasks without delivered code.

### Gap D: Security risk (hardcoded secrets)
- `juggernaut-autonomy/core/mcp_factory.py` contains hardcoded secrets (Neon connection string and Railway identifiers/tokens).
- These should be treated as exposed and rotated; move to environment variables.

---

## Recommended Priority Fixes (wire + tighten)

### Priority 0: Tighten completion evidence for code tasks
Goal: `completed` must mean “merged” (or at least “delivered”) for code tasks.

Clean tightening points:
- `core/verification.py`: make PR URLs insufficient for completion for code/github task types unless merged.
- `main.py:update_task_status(...)`: if evidence is only `pr_created`, block `completed` and force `awaiting_pr_merge`.

### Priority 1A: Expose missing write tools in Brain tool schema
Goal: Neural Chat can actually produce code artifacts via MCP.

- Add `github_put_file` and (optionally) `github_merge_pr` to `core/mcp_tool_schemas.py`.

### Priority 1B (preferred over 1A long-term): Add a single “code executor” tool wrapper
Goal: one tool call triggers the proven code pipeline.

- Expose a `code_task_execute` tool that calls `src/task_executor_code.execute_code_task(...)`.
- This avoids the LLM doing low-level multi-file commits itself.

### Priority 2: Unify stacks and make one path authoritative
Goal: reduce drift and increase reliability.

- Decide whether the authoritative mechanism is:
  - code executor pipeline, or
  - MCP file-write primitives.
- Make the other call into it.

---

## Summary
JUGGERNAUT already has:
- A working code generation + PR creation pipeline (engine path).
- A PR monitoring job that can complete tasks only after merge.

The autonomous builder gap is not “missing codegen”. It is:
- tool exposure (Neural Chat cannot call the write actions)
- completion semantics (`pr_created` accepted as valid for completion)
- two parallel stacks not unified.
