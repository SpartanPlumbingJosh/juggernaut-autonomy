# Milestone 4: GitHub Code Crawler - Architecture

## Goal
Proactive code health monitoring. Analyze codebase for issues, detect stale code, validate API contracts, and automatically create PRs for mechanical fixes.

## Components

### 1. GitHub API Client (`core/github_client.py`)
**Purpose:** Interact with GitHub API for repo analysis and PR creation

**Capabilities:**
- Authenticate with GitHub token
- Fetch repository contents
- Read file contents
- Create branches
- Create pull requests
- Add PR comments
- List open PRs

**API Endpoints:**
- `GET /repos/{owner}/{repo}/contents/{path}`
- `POST /repos/{owner}/{repo}/git/refs`
- `POST /repos/{owner}/{repo}/pulls`
- `GET /repos/{owner}/{repo}/pulls`

### 2. Code Analyzers

#### A. Stale Code Detector (`core/analyzers/stale_code.py`)
**Detects:**
- Unused imports
- Unused functions/classes
- Dead code paths
- Commented-out code blocks
- TODO/FIXME comments older than 30 days

**Algorithm:**
1. Parse Python files with AST
2. Build symbol table (all definitions)
3. Build usage table (all references)
4. Find symbols defined but never used
5. Detect unreachable code after return/raise
6. Find commented code blocks

#### B. Contract Validator (`core/analyzers/contract_validator.py`)
**Detects:**
- Frontend expects field that backend doesn't return
- Backend returns field that frontend doesn't use
- Type mismatches (string vs number)
- Missing required fields
- Deprecated API usage

**Algorithm:**
1. Parse backend API responses (from code or OpenAPI spec)
2. Parse frontend API calls (TypeScript interfaces)
3. Compare expected vs actual fields
4. Flag mismatches and missing fields

#### C. Dependency Checker (`core/analyzers/dependency_checker.py`)
**Detects:**
- Outdated packages (compare to latest versions)
- Security vulnerabilities (via GitHub Advisory Database)
- Unused dependencies
- Conflicting version requirements

**Algorithm:**
1. Parse requirements.txt / package.json
2. Check each package against PyPI/npm
3. Query GitHub Advisory Database
4. Find packages imported in code vs declared

### 3. PR Creator (`core/pr_creator.py`)
**Purpose:** Automatically create PRs for mechanical fixes

**Safe Fixes (Auto-PR):**
- Remove unused imports
- Remove trailing whitespace
- Fix indentation issues
- Update outdated dependencies (patch versions only)
- Add missing type hints (simple cases)

**Unsafe Fixes (Create Task):**
- Remove unused functions (might be public API)
- Change API contracts
- Update major versions
- Refactor code structure

**PR Template:**
```markdown
## Automated Code Health Fix

**Type:** [Unused Import Removal / Dependency Update / etc]
**Confidence:** High
**Risk:** Low

### Changes
- Removed unused import `foo` from `bar.py`
- Updated `requests` from 2.28.0 to 2.28.2

### Verification
- [x] All tests pass
- [x] No breaking changes
- [x] Follows style guide

**Generated by:** JUGGERNAUT Code Crawler
**Review required:** Yes (always require human approval)
```

### 4. Code Health Scorer (`core/code_health_scorer.py`)
**Purpose:** Calculate overall code health score

**Metrics:**
- **Staleness:** % of unused code
- **Contract Health:** % of API mismatches
- **Dependency Health:** % of outdated/vulnerable packages
- **Documentation:** % of functions with docstrings
- **Test Coverage:** % of code covered by tests (if available)

**Score Calculation:**
```
Health Score = (
    0.3 * (1 - staleness_ratio) +
    0.3 * (1 - contract_mismatch_ratio) +
    0.2 * (1 - outdated_dependency_ratio) +
    0.1 * documentation_ratio +
    0.1 * test_coverage_ratio
) * 100
```

### 5. Code Crawler Scheduler (`core/code_crawler.py`)
**Purpose:** Run code analysis on schedule

**Flow:**
1. Clone/pull latest code from GitHub
2. Run all analyzers in parallel
3. Store findings in database
4. Calculate health score
5. Create PRs for safe fixes
6. Create tasks for unsafe fixes
7. Update crawler state

**Schedule:** Every 6 hours or on-demand

## Database Schema

### `code_analysis_runs` Table
```sql
CREATE TABLE code_analysis_runs (
    id UUID PRIMARY KEY,
    repository VARCHAR(200),
    branch VARCHAR(100),
    commit_sha VARCHAR(40),
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    status VARCHAR(50),
    health_score DECIMAL(5,2),
    findings_count INTEGER,
    prs_created INTEGER,
    tasks_created INTEGER,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

### `code_findings` Table
```sql
CREATE TABLE code_findings (
    id UUID PRIMARY KEY,
    run_id UUID REFERENCES code_analysis_runs(id),
    finding_type VARCHAR(50),
    severity VARCHAR(20),
    file_path VARCHAR(500),
    line_number INTEGER,
    description TEXT,
    suggestion TEXT,
    auto_fixable BOOLEAN,
    fixed BOOLEAN DEFAULT FALSE,
    pr_id VARCHAR(100),
    created_at TIMESTAMP DEFAULT NOW()
);
```

### `api_contracts` Table
```sql
CREATE TABLE api_contracts (
    id UUID PRIMARY KEY,
    endpoint VARCHAR(200),
    method VARCHAR(10),
    backend_schema JSONB,
    frontend_schema JSONB,
    mismatches JSONB,
    last_validated TIMESTAMP,
    status VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);
```

### `dependency_status` Table
```sql
CREATE TABLE dependency_status (
    id UUID PRIMARY KEY,
    package_name VARCHAR(200),
    current_version VARCHAR(50),
    latest_version VARCHAR(50),
    is_outdated BOOLEAN,
    has_vulnerabilities BOOLEAN,
    vulnerability_details JSONB,
    last_checked TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW()
);
```

## API Endpoints

### Backend (`juggernaut-autonomy`)
- `POST /api/code/analyze` - Trigger code analysis
- `GET /api/code/runs` - Get analysis runs
- `GET /api/code/runs/{id}` - Get run details
- `GET /api/code/findings` - Get findings
- `GET /api/code/health` - Get health score
- `GET /api/code/contracts` - Get API contracts

### Frontend (`spartan-hq`)
- Proxy through Next.js API routes
- `/api/code/*` â†’ Backend

## Analysis Types

### 1. Static Analysis
- AST parsing for Python
- TypeScript AST for frontend
- Regex for simple patterns
- No code execution

### 2. Contract Analysis
- Parse API route definitions
- Parse TypeScript interfaces
- Compare schemas
- Detect mismatches

### 3. Dependency Analysis
- Parse package manifests
- Query package registries
- Check security advisories
- Find unused packages

## PR Creation Strategy

### Auto-Create PR (Low Risk)
1. Remove unused imports
2. Fix whitespace/formatting
3. Update patch versions
4. Add missing docstrings

### Create Task (High Risk)
1. Remove unused functions
2. Update major versions
3. Change API contracts
4. Refactor code

### Never Auto-Fix
1. Logic changes
2. Algorithm modifications
3. Security-related code
4. Database migrations

## Success Metrics

1. **Detection Accuracy:** 95%+ true positives
2. **PR Quality:** 90%+ PRs merged without changes
3. **Health Score Improvement:** +5 points per month
4. **False Positives:** < 5% of findings
5. **Analysis Speed:** < 5 minutes for full repo

## Phase 1 (Tonight - 2 hours)
1. Database schema
2. GitHub API client
3. Stale code detector (basic)
4. Code Health UI (basic)

## Phase 2 (Next Session - 2 hours)
1. Contract validator
2. Dependency checker
3. PR creator
4. Health scorer

## Phase 3 (Next Session - 1 hour)
1. Code crawler scheduler
2. Integration testing
3. Deployment

## Dependencies

**Required:**
- GitHub token (GITHUB_TOKEN env var)
- Repository access (read + write)
- Python AST library (built-in)

**Optional:**
- TypeScript parser (for frontend analysis)
- OpenAPI spec (for contract validation)
- Test coverage data

## Security

- GitHub token stored in env var
- Rate limit GitHub API calls
- Never commit sensitive data
- All PRs require human approval
- No automatic merges

## Future Enhancements

- ML-based code smell detection
- Performance regression detection
- Security vulnerability scanning
- Automated test generation
- Code complexity metrics
