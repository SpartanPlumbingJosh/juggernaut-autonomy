# ==============================================================================
# JUGGERNAUT LiteLLM Proxy Configuration
# ==============================================================================
# Multi-provider routing: OpenRouter (primary) → OpenAI (fallback)
# Both share model_name "openrouter/auto" so LiteLLM fails over automatically.
# Workers request "openrouter/auto" — no code changes needed.
# ==============================================================================

model_list:
  # ── OpenRouter (primary — existing provider, smart routing) ──
  - model_name: "openrouter/auto"
    litellm_params:
      model: "openrouter/auto"
      api_key: "os.environ/OPENROUTER_API_KEY"
      api_base: "https://openrouter.ai/api/v1"
      max_tokens: 8192
    model_info:
      description: "Primary — OpenRouter smart routing"

  # ── OpenAI direct (fallback 1 — no markup) ──
  - model_name: "openrouter/auto"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
      max_tokens: 8192
    model_info:
      description: "Fallback 1 — OpenAI direct (no markup)"

  - model_name: "openai/gpt-4o"
    litellm_params:
      model: "openai/gpt-4o"
      api_key: "os.environ/OPENAI_API_KEY"
      max_tokens: 8192

  - model_name: "openai/gpt-4o-mini"
    litellm_params:
      model: "openai/gpt-4o-mini"
      api_key: "os.environ/OPENAI_API_KEY"
      max_tokens: 4096

litellm_settings:
  num_retries: 2
  retry_after: 5
  request_timeout: 120
  # Budget controls
  max_budget: 50.0
  budget_duration: "1d"
  set_verbose: false

general_settings:
  # master_key: "os.environ/LITELLM_MASTER_KEY"  # Re-enable when adding auth
  health_check: true
  allow_routes_without_auth:
    - "/health"
    - "/health/liveliness"
    - "/health/readiness"
